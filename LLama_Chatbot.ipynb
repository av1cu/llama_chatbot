{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvj-QTc_1Lmn"
      },
      "outputs": [],
      "source": [
        " !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain llama-cpp-python gradio huggingface_hub"
      ],
      "metadata": {
        "id": "mHbb1_7z4HtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "pw2_9Jzx_2i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama_cpp + huggingface_hub + gradio"
      ],
      "metadata": {
        "id": "uCVcjr2c-9QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "from huggingface_hub import hf_hub_download\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "5_lT1FE1EpDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(\n",
        "  repo_id='issai/LLama-3.1-KazLLM-1.0-8B-GGUF4',\n",
        "  filename='checkpoints_llama8b_031224_18900-Q4_K_M.gguf'\n",
        ")"
      ],
      "metadata": {
        "id": "ihl-Vka6FYAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(model_path, n_ctx=2048, n_gpu_layers=35)"
      ],
      "metadata": {
        "id": "5pQdv9NLH2Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(question, history):\n",
        "  formatted_prompt = f\"[INST] Сен көмекшісің. Қазақ тілінде жауап бер. {question} [/INST]\"\n",
        "  output = llm(formatted_prompt, max_tokens=256, stop=[\"</s>\", \"[INST]\"])\n",
        "  return output[\"choices\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "epLxCkPHKZAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(\n",
        "    fn=response,\n",
        "    type=\"messages\"\n",
        ").launch(debug=True)"
      ],
      "metadata": {
        "id": "Jxs1snZIJ61r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using langchain"
      ],
      "metadata": {
        "id": "6tHZa84P_M9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "uCZ6N7xCGgK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain"
      ],
      "metadata": {
        "id": "h5sjQYSI_Gn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LlamaCpp(\n",
        "  model_path=model_path,\n",
        "  n_ctx=2048,\n",
        "  temperature=0.7,\n",
        ")"
      ],
      "metadata": {
        "id": "qJHSUVir_S7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory()"
      ],
      "metadata": {
        "id": "JVaZAY10BzgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ConversationChain(\n",
        "  llm=llm,\n",
        "  memory=memory,\n",
        "  verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "IfXP3Xb2B04x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_kazakh_prompt(question):\n",
        "  return f\"[INST] Сен көмекшісің. Қазақ тілінде жауап бер: {question} [/INST]\""
      ],
      "metadata": {
        "id": "P9YfWkV0Ch8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response_langchain(question, history):\n",
        "  prompt_question = get_kazakh_prompt(question)\n",
        "  response = chat.predict(input=prompt_question)\n",
        "  return response"
      ],
      "metadata": {
        "id": "ad0iGTwqCmVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(\n",
        "  fn=response_langchain,\n",
        "  type=\"messages\"\n",
        ").launch(debug=True)"
      ],
      "metadata": {
        "id": "ctAqY4IZC5px"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}